{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932396f5",
   "metadata": {},
   "source": [
    "# Régression logistique, perte logistique\n",
    "\n",
    "Dans ce TP, vous allez :\n",
    "- explorer la raison pour laquelle la perte d'erreur au carré n'est pas appropriée pour la régression logistique\n",
    "- explorer la fonction de perte logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
    "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147857b7",
   "metadata": {},
   "source": [
    "## Erreur au carré pour la régression logistique\n",
    "<img align=\"left\" src=\"./images/C1_W3_SqErrorVsLogistic.png\"     style=\" width:400px; padding: 10px; \" > Rappelez-vous que pour la régression **linéaire**, nous avons utilisé la fonction de coût d'**erreur au carré** :\n",
    "L'équation pour le coût d'erreur au carré avec une variable est :\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "où \n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099a6a4",
   "metadata": {},
   "source": [
    "Rappelez-vous, le coût de l'erreur au carré avait la belle propriété que suivre la dérivée du coût mène au minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7192ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bowl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddcf3b",
   "metadata": {},
   "source": [
    "Cette fonction de coût a bien fonctionné pour la régression linéaire, il est donc naturel de l'envisager pour la régression logistique également. Cependant, comme le montre le block ci-dessus, $f_{wb}(x)$ a maintenant une composante non linéaire, la fonction sigmoïde : $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$. Essayons une erreur au carré sur l'exemple d'un TP précédent, en incluant maintenant la sigmoïde.\n",
    "\n",
    "Voici nos données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
    "plt_simple_example(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41229728",
   "metadata": {},
   "source": [
    "Now, let's get a surface plot of the cost using a *squared error cost*:\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ \n",
    " \n",
    "where \n",
    "  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4671a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt_logistic_squared_error(x_train,y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807da91",
   "metadata": {},
   "source": [
    "Bien que cela produise un graphique assez intéressant, la surface ci-dessus n'est pas aussi lisse que le \"bol de soupe\" de la régression linéaire !\n",
    "\n",
    "La régression logistique nécessite une fonction de coût plus adaptée à sa nature non linéaire. Cela commence par une fonction de perte. Cela est décrit ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e92bf4",
   "metadata": {},
   "source": [
    "## Fonction de perte logistique\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_a.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_b.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_c.png\"     style=\" width:250px; padding: 2px; \" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65beda",
   "metadata": {},
   "source": [
    "La régression logistique utilise une fonction de perte plus adaptée à la tâche de catégorisation où la cible est 0 ou 1 plutôt que n'importe quel nombre.\n",
    "\n",
    ">**Note de définition :**   Dans ce cours, ces définitions sont utilisées :  \n",
    "**Perte** est une mesure de la différence d'un seul exemple à sa valeur cible tandis que le  \n",
    "**Coût** est une mesure des pertes sur l'ensemble d'entraînement\n",
    "\n",
    "\n",
    "Cela est défini :\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ est le coût pour un seul point de données, qui est :\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{si $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{si $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ est la prédiction du modèle, tandis que $y^{(i)}$ est la valeur cible.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ où la fonction $g$ est la fonction sigmoïde.\n",
    "\n",
    "La caractéristique définissante de cette fonction de perte est le fait qu'elle utilise deux courbes séparées. Une pour le cas où la cible est zéro ou ($y=0$) et une autre pour quand la cible est un ($y=1$). Combinées, ces courbes fournissent le comportement utile pour une fonction de perte, à savoir, être zéro lorsque la prédiction correspond à la cible et augmenter rapidement en valeur lorsque la prédiction diffère de la cible. Considérez les courbes ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e58273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plt_logistic_loss import plt_two_logistic_loss_curves\n",
    "plt_two_logistic_loss_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af27eb3",
   "metadata": {},
   "source": [
    "Combinées, les courbes sont similaires à la courbe quadratique de la perte d'erreur au carré. Notez que l'axe des x est $f_{\\mathbf{w},b}$ qui est la sortie d'une sigmoïde. La sortie de la sigmoïde est strictement comprise entre 0 et 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34ac60",
   "metadata": {},
   "source": [
    "La fonction de perte ci-dessus peut être réécrite pour être plus facile à mettre en œuvre.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "Cette équation n'est pas très jolie... Elle est moins intimidante lorsque vous considérez que $y^{(i)}$ ne peut avoir que deux valeurs, 0 et 1. On peut alors considérer l'équation en deux parties :  \n",
    "quand $ y^{(i)} = 0$, le terme de gauche est éliminé :\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "et quand $ y^{(i)} = 1$, le terme de droite est éliminé :\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Avec cette nouvelle fonction de perte logistique, une fonction de coût peut être produite, et qui intègre la perte de tous les exemples. Ce sera le sujet du prochain TP. Pour l'instant, jetons un coup d'œil à la courbe du coût par rapport aux paramètres pour l'exemple simple que nous avons considéré ci-dessus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6a199",
   "metadata": {},
   "source": [
    "Cette courbe est bien adaptée à la descente de gradient ! Elle n'a pas de plateaux, de minima locaux, ou de discontinuités. Notez qu'elle n'est pas une cuvette comme dans le cas de l'erreur au carré. Le coût et le logarithme du coût sont tous deux tracés pour mettre en évidence le fait que la courbe, lorsque le coût est faible, a une pente et continue à diminuer. Rappel : vous pouvez faire pivoter les tracés ci-dessus avec votre souris."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
