{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c5d187",
   "metadata": {},
   "source": [
    "# Goals\n",
    "The notebook introduces you to pandas dataframes and their use cases. You can browse the pandas <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\">documentation</a> to learn about their various functionalities.\n",
    "\n",
    "It also introduces you to different transformation methods for data. Moreover, a few examples of accuracy analysis and trend smoothing are shown here.\n",
    "\n",
    "You are expected to read the code and understand how it works, and then complete it when necessary following the inline suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e13607",
   "metadata": {},
   "source": [
    "# Sqrt Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1186464-9849-4779-8760-58e59c0aeb27",
   "metadata": {},
   "source": [
    "In the cell below, Australian monthly electricity data is loaded, and the sqrt transformation methods is applied. The time plot of the transformed data is shown, and then also a corresponding histogram is extracted, to highlight changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8425aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "from pandas import DataFrame\n",
    "from pandas import read_excel\n",
    "from matplotlib import pyplot\n",
    "series = read_excel('Electricity.xls',\n",
    "              sheet_name='Data', header=0,\n",
    "              index_col=0, parse_dates=True)\n",
    "dataframe = DataFrame(series.values)\n",
    "dataframe.columns = ['electricity']\n",
    "\n",
    "# sqrt transformation is applied:\n",
    "dataframe['electricity'] = sqrt(dataframe['electricity'])\n",
    "pyplot.figure(1)\n",
    "# line plot\n",
    "pyplot.subplot(211)\n",
    "pyplot.plot(dataframe['electricity'])\n",
    "# histogram\n",
    "pyplot.subplot(212)\n",
    "pyplot.hist(dataframe['electricity'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1365e",
   "metadata": {},
   "source": [
    "# Log Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c3706-14b0-4c3e-86c9-f47b42fc3c9c",
   "metadata": {},
   "source": [
    "Similairly, log transformation is applied and the time plot and histogram are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10698b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "series = read_excel('Electricity.xls',\n",
    "              sheet_name='Data', header=0,\n",
    "              index_col=0, parse_dates=True)\n",
    "dataframe = DataFrame(series.values)\n",
    "dataframe.columns = ['electricity']\n",
    "dataframe['electricity'] = log(dataframe['electricity'])\n",
    "pyplot.figure(1)\n",
    "# line plot\n",
    "pyplot.subplot(211)\n",
    "pyplot.plot(dataframe['electricity'])\n",
    "# histogram\n",
    "pyplot.subplot(212)\n",
    "pyplot.hist(dataframe['electricity'])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf343b6c-7af8-47d6-8982-b61c86992e98",
   "metadata": {},
   "source": [
    "In the cell below, please use the `hist` method of pandas (documentaton is <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html\">here</a>) to plot the \"electricity\" column for the dataframe loaded in the cell above. Compare it with what you obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79468155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d6850-6bb4-4728-91d1-04af4f9a728b",
   "metadata": {},
   "source": [
    "Here below, convert the \"electricity\" column of the previous pandas dataframe into a numpy array (you have seen how to do so last week), and then apply the z-score normalization as you've learned when we discussed feature scaling. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8443080",
   "metadata": {},
   "source": [
    "# Calendar adjustments\n",
    "If data is for calendar months, then account might have to be taken of the length of a month. The difference between the longest and shortest months is about $\\frac{(31- 28)}{30} = 10\\%$. The adjustment needed is\n",
    "\n",
    "$$\n",
    "W_t = \\frac{\\text{# of days in an average month}}{\\text{# of days in month } i} \\times Y_t = \\frac{365.25/12}{\\text{# of days in month } i} \\times Y_t\n",
    "$$\n",
    "\n",
    "The code below loads the original and the adjusted data from an excel spreadsheet containing milk production data, and plots them on top of each other. Open the excel sheet and check manually how the adjustment was computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = read_excel('MilkProduction.xls',\n",
    "              sheet_name='AdjustedData', header=0,\n",
    "              index_col=0, parse_dates=True)  # you can include various other parameters\n",
    "series.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d82182",
   "metadata": {},
   "source": [
    "# Accuracy Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d24fcf",
   "metadata": {},
   "source": [
    "The forecaster needs to choose the best model to use for forecasting any particular time series. We discuss here different measures for comparing different forecasting models on the basis of forecasting errors. Let $F_t$ be the forecast value and $Y_t$ be the actual observation at time $t$. Then the forecast error at time $t$ is defined as $e_t = Y_t - F_t$.\n",
    "\n",
    "\n",
    "Usually $F_t$ is calculated from previous values of $Y_t$ right up to and including the immediate preceding value $Y_{t-1}$. Thus $F_t$ predicts just one step ahead. In this case $F_t$ is called the **one-step forecast** and $e_t$ is called the **one-step forecast error**. Usually we assess error not from one such $e_t$ but from $n$ values. Three measures of error are:\n",
    "\n",
    "* **Mean Error (ME)**:\n",
    "$$\n",
    "ME = \\frac{1}{n} \\sum_{t=1}^{n} e_t\n",
    "$$\n",
    "\n",
    "* **Mean Absolute Error (MAE)**:\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{t=1}^{n} |e_t|\n",
    "$$\n",
    "\n",
    "* **Mean Square Error (MSE)**:\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{t=1}^{n} e_t^2\n",
    "$$\n",
    "\n",
    "Below, Australian beer production data is analyzed, loading the predictions of two naive forecasts methods labeled as NF1 and NF2, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AustralianBeer  = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols = [1],\n",
    "                             header=0,  dtype=float)\n",
    "AustralianBeer = AustralianBeer['Original Data']\n",
    "NaiveF1  = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols = [2],\n",
    "                      header=0,  dtype=float)\n",
    "NaiveF1 = NaiveF1['NF1']\n",
    "NaiveF2 = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols=[3],\n",
    "                     header=0,  dtype=float)\n",
    "NaiveF2 = NaiveF2['NF2']\n",
    "\n",
    "# Joint plot of original data and NF1 forecasts\n",
    "AustralianBeer.plot(legend=True)\n",
    "NaiveF1.plot(legend=True)\n",
    "pyplot.show()\n",
    "\n",
    "# Joint plot of original data and NF2 forecasts\n",
    "AustralianBeer.plot(legend=True)\n",
    "NaiveF2.plot(legend=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c214b-c3d3-4c28-8095-0ef07353dc64",
   "metadata": {},
   "source": [
    "Complete the cell below to compute the error for the \"NaiveF1\" and \"NaiveF2\" models with respect to the original \"AustralianBeer\" data, and then calculate the ME, MAE, MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d388616-d852-4493-bd19-3970e6cfe671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the code here\n",
    "Error1 = \n",
    "Error2 = \n",
    "ME1 = \n",
    "ME2 = \n",
    "MAE1=\n",
    "MAE2=\n",
    "MSE1=\n",
    "MSE2="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03ecbe-d6ab-44f4-8eea-058dc08462ac",
   "metadata": {},
   "source": [
    "Similarly, the percent errors are computed here below, and then all the errors are displayed in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5ed08-77ff-4af1-8f05-877705d38e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PercentageError1=(Error1/AustralianBeer)*100\n",
    "PercentageError2=(Error2/AustralianBeer)*100\n",
    "MPE1 = sum(PercentageError1)* 1.0/len(NaiveF1)\n",
    "MPE2 = sum(PercentageError2)* 1.0/len(NaiveF2)\n",
    "MPAE1= sum(abs(PercentageError1))*1.0/len(NaiveF1)\n",
    "MPAE2= sum(abs(PercentageError2))*1.0/len(NaiveF2)\n",
    "\n",
    "#Printing a summary of the errors in a tabular form\n",
    "print('Summary of errors resulting from NF1 & NF2:')\n",
    "import pandas as pd\n",
    "cars = {'Errors': ['ME','MAE','MSE','MPE', 'MAPE'],\n",
    "        'NF1': [ME1, MAE1, MSE1, MPE1, MPAE1],\n",
    "        'NF2': [ME2, MAE2, MSE2, MPE2, MPAE2]\n",
    "        }\n",
    "AllErrors = pd.DataFrame(cars, columns = ['Errors', 'NF1', 'NF2'])\n",
    "print(AllErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded2e34",
   "metadata": {},
   "source": [
    "# ACF of forecast error\n",
    "It is often useful to regard the one-step forecast errors as a time series in its own right, and to calculate and plot the autocorrelation function (ACF) of this series. As mentioned above in the context\n",
    "of the errors, one would want the errors generated by a forecast method to be completely/purely\n",
    "random. If that is not the case, the ACF can retain some patterns observable in the original data set.\n",
    "Hence, having the ACF not completely random can be an indication that the forecasting method is\n",
    "not necessarily accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AustralianBeer  = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols = [1],\n",
    "                             header=0,  dtype=float)\n",
    "AustralianBeer =AustralianBeer['Original Data']\n",
    "\n",
    "NaiveF1  = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols = [2],\n",
    "                      header=0, dtype=float)\n",
    "NaiveF1 = NaiveF1['NF1']\n",
    "# same as above\n",
    "NaiveF2 = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols=[3],\n",
    "                     header=0, dtype=float)\n",
    "NaiveF2 = NaiveF2['NF2']\n",
    "\n",
    "# Plot for the original data set\n",
    "AustralianBeer.plot(label='Original data', legend=True)\n",
    "pyplot.show()\n",
    "\n",
    "# Evaluating the errors from both NF1 and NF2 methods\n",
    "Error1 = AustralianBeer - NaiveF1\n",
    "Error2 = AustralianBeer - NaiveF2\n",
    "\n",
    "# Plot of the error time series\n",
    "Error1.plot(label='NF1 error plot', legend=True)\n",
    "Error2.plot(label='NF2 error plot', legend=True)\n",
    "pyplot.show()\n",
    "\n",
    "# Creating an autocorrelation plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(Error1)\n",
    "autocorrelation_plot(Error2)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dfb20",
   "metadata": {},
   "source": [
    "# Prediction Interval\n",
    "\n",
    "Assuming that the errors are normally distributed, one can usefully assess the accuracy of a forecast by using MSE (Mean Squared Error) as an estimate of the error. An approximate prediction interval for the next observation is:\n",
    "\n",
    "$$F_{t+1} ± z \\sqrt{MSE},$$\n",
    "\n",
    "where $z$ is a quantile of the normal distribution. Typical values used are:\n",
    "\n",
    "| $z$   | Probability |\n",
    "|-------|-------------|\n",
    "| 1.282 | 0.80        |\n",
    "| 1.645 | 0.90        |\n",
    "| 1.960 | 0.95        |\n",
    "| 2.576 | 0.99        |\n",
    "\n",
    "This enables, for example, 95% or 99% confidence intervals to be set up for any forecast.\n",
    "\n",
    "In the cell below, the fisrt column of the \"NF1NF2\" sheet is loaded using the `usecols` argument of the 'read_excel' function, retrieving the \"AustralianBeer\" original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b10e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "AustralianBeer  = read_excel('BeerErrorsData.xls', sheet_name='NF1NF2', usecols = [1],\n",
    "                             header=0,  dtype=float)\n",
    "AustralianBeer = AustralianBeer['Original Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b07716-af09-447f-a5a6-9d8764ae2ed1",
   "metadata": {},
   "source": [
    "Now, following the above examples, load at first the second column, and then the third column, and assign them to the \"NaiveF1\" and \"NaiveF2\" dataframes, respectively. Make sure by opening the spreadsheet that you are loading the correct column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289745a5-d60c-424b-b71b-24ff2d16a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the four code lines below\n",
    "NaiveF1 = read_excel()\n",
    "NaiveF1 = \n",
    "NaiveF2 = read_excel()\n",
    "NaiveF2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0a256-8847-4182-b1d4-c69c93cc49d5",
   "metadata": {},
   "source": [
    "Finally, calculate the 0.90 confidence interval (so, z=1.645) for the \"NaiveF1\" and \"NaiveF2\" forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5ef2f-e96b-4a94-abb2-0e21cd988d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the code below\n",
    "Error1 = \n",
    "Error2 = \n",
    "MSE1=\n",
    "MSE2=\n",
    "\n",
    "LowerForecast1 = NaiveF1 - \n",
    "UpperForecast1 = NaiveF1 + \n",
    "\n",
    "LowerForecast2 = NaiveF2 - \n",
    "UpperForecast2 = NaiveF2 + "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97aa72-a42e-4775-8b58-6fd477a556c7",
   "metadata": {},
   "source": [
    "The results are then plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba1a57-2090-496e-b957-32a8d9e5444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint plot of original data and NF1 forecasts\n",
    "AustralianBeer.plot(label='Original data')\n",
    "NaiveF1.plot(label='NF1 forecast')\n",
    "LowerForecast1.plot(label='NF1 lower bound')\n",
    "UpperForecast1.plot(label='NF1 upper bound')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# Joint plot of original data and NF2 forecasts\n",
    "AustralianBeer.plot(label='Original data')\n",
    "NaiveF2.plot(label='NF2 forecast')\n",
    "LowerForecast2.plot(label='NF2 lower bound')\n",
    "UpperForecast2.plot(label='NF2 upper bound')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708298cf",
   "metadata": {},
   "source": [
    "# Exponential smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7ef66",
   "metadata": {},
   "source": [
    "# Single Exponential Smoothing\n",
    "\n",
    "The single exponential forecast or simple exponential smoothing (SES) is defined as:\n",
    "\n",
    "$$F_{t+1} = \\alpha Y_t + (1 - \\alpha)F_t$$\n",
    "\n",
    "where $\\alpha$ is a given weight value to be selected subject to $0 \\leq \\alpha \\leq 1$. Thus $F_{t+1}$ is the weighted average of the current observation, $Y_t$, with the forecast, $F_t$, made at the previous time point $t - 1$.\n",
    "\n",
    "Repeated application of the formula yields:\n",
    "\n",
    "$$F_{t+1} = (1 - \\alpha)^t F_1 + \\alpha \\sum_{j=0}^{t-1} (1 - \\alpha)^j Y_{t-j}$$\n",
    "\n",
    "showing that the dependence of the current forecast on $Y_t$, $Y_{t-1}$, $Y_{t-2}$, ..., falls away in an exponential way. The rate at which this dependence falls away is controlled by $\\alpha$. The larger the value of $\\alpha$, the quicker does the dependence on previous values fall away.\n",
    "\n",
    "SES needs to be initialized. A simple choice is to use $F_1 = Y_1$. Other values are possible, but we shall not agonise over this too much as we are more concerned with the behaviour of the forecast once it has been in use for a while.\n",
    "\n",
    "It should be noted that, as with averaging methods, SES can only produce a one-step forecast.\n",
    "\n",
    "Below, shampoo sales data is loaded and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "series = read_excel('ShampooSales.xls', sheet_name='Data', header=0,\n",
    "              index_col=0, parse_dates=True)\n",
    "series.index.freq = 'MS'\n",
    "\n",
    "# Simple Exponential Smoothing #\n",
    "\n",
    "## SES model 1: alpha = 0.1\n",
    "fit1 = SimpleExpSmoothing(series).fit(smoothing_level=0.1,optimized=False)\n",
    "fcast1 = fit1.forecast(10).rename(r'$\\alpha=0.1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71b1f7-4e54-482d-b469-9c8c32c73c44",
   "metadata": {},
   "source": [
    "Below, define \"fit2\" using alpha = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57240b44-f231-4344-b509-ecdbc4167369",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SES model 2: alpha = 0.7\n",
    "fit2 = \n",
    "fcast2 = fit2.forecast(10).rename(r'$\\alpha=0.7$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a116a-8db5-4bee-ac77-ac6f48326aba",
   "metadata": {},
   "source": [
    "Finally,  we show an example of alpha automatically selected by the built-in optimization software, and plot the three forecats methods together with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389f5c0-78e7-452d-816e-10894e5b1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SES model 3: alpha automatically selected by the built-in optimization software\n",
    "fit3 = SimpleExpSmoothing(series).fit()\n",
    "fcast3 = fit3.forecast(10).rename(r'$\\alpha=%s$'%fit3.model.params['smoothing_level'])\n",
    "\n",
    "# Plotting the original data\n",
    "series.plot(color='black', legend=True)\n",
    "\n",
    "# Plot of fitted values and forecast of next 10 values, respectively, for the three models\n",
    "fit1.fittedvalues.plot(color='blue')\n",
    "fcast1.plot(color='blue', legend=True)\n",
    "fit2.fittedvalues.plot(color='red')\n",
    "fcast2.plot(color='red', legend=True)\n",
    "fcast3.plot(color='green', legend=True)\n",
    "fit3.fittedvalues.plot(color='green')\n",
    "pyplot.show()\n",
    "\n",
    "#Evaluating the errors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE1=mean_squared_error(fit1.fittedvalues, series)\n",
    "MSE2=mean_squared_error(fit2.fittedvalues, series)\n",
    "MSE3=mean_squared_error(fit3.fittedvalues, series)\n",
    "\n",
    "print('Summary of errors resulting from SES models 1, 2 & 3:')\n",
    "import pandas as pd\n",
    "cars = {'Model': ['MSE'],\n",
    "        'SES model 1': [MSE1],\n",
    "        'SES model 2': [MSE2],\n",
    "        'SES model 3': [MSE3]\n",
    "        }\n",
    "AllErrors = pd.DataFrame(cars, columns = ['Model', 'SES model 1', 'SES model 2', 'SES model 3'])\n",
    "print(AllErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fbbc8",
   "metadata": {},
   "source": [
    "# Holt’s Linear Exponential Smoothing\n",
    "\n",
    "The Holt method, also known as the linear exponential method (LES), was introduced by Charles Holt in 1957. It is an extension of the simple/single exponential smoothing method to take into account a possible (local) linear trend. The trend makes it possible to forecast m time periods ahead.\n",
    "\n",
    "There are two smoothing constants $\\alpha$ and $\\beta$, $0 \\leq \\alpha, \\beta \\leq 1$. The equations are:\n",
    "\n",
    "$$L_t = \\alpha Y_t + (1 - \\alpha)(L_{t-1} + b_{t-1})$$\n",
    "$$b_t = \\beta (L_t - L_{t-1}) + (1 - \\beta)b_{t-1}$$\n",
    "$$F_{t+m} = L_t + b_t m$$\n",
    "\n",
    "Here $L_t$ and $b_t$ are respectively (exponentially smoothed) estimates of the level and linear trend of the series at time t, whilst $F_{t+m}$ is the linear forecast from t onwards.\n",
    "\n",
    "Initial estimates are needed for $L_1$ and $b_1$. Simple choices are $L_1 = Y_1$ and $b_1 = 0$. If, however, zero is atypical of the initial slope, then a more careful estimate of the slope may be needed to ensure that the initial forecasts are not badly out.\n",
    "\n",
    "It should be noted that to use this method, it is **not** necessary for a series to be completely linear, but some trend should be present.\n",
    "\n",
    "Below, the same shampoo sales data of the previous example is loaded and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import Holt\n",
    "\n",
    "series = read_excel('ShampooSales.xls', sheet_name='Data', header=0,\n",
    "              index_col=0)\n",
    "series.index.freq = 'MS'\n",
    "\n",
    "# Holt model 1: alpha = 0.8, beta=0.2\n",
    "fit1 = Holt(series).fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n",
    "fcast1 = fit1.forecast(12).rename(\"Holt's linear trend\")\n",
    "\n",
    "fit2 = Holt(series, exponential=True).fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n",
    "fcast2 = fit2.forecast(12).rename(\"Exponential trend\")\n",
    "\n",
    "fit3 = Holt(series, damped_trend=True).fit(smoothing_level=0.8, smoothing_trend=0.2)\n",
    "fcast3 = fit3.forecast(12).rename(\"Additive damped trend\")\n",
    "\n",
    "fit4 = Holt(series).fit(optimized=True)\n",
    "fcast4 = fit4.forecast(12).rename(\"Additive 2 damped trend\")\n",
    "\n",
    "#Plotting original data and the four forecast methods\n",
    "series.plot(color='black', legend=True)\n",
    "fit1.fittedvalues.plot(color='blue')\n",
    "fcast1.plot(color='blue', legend=True)\n",
    "fit2.fittedvalues.plot(color='red')\n",
    "fcast2.plot(color='red', legend=True)\n",
    "fit3.fittedvalues.plot(color='green')\n",
    "fcast3.plot(color='green', legend=True)\n",
    "fcast4.plot(color='yellow', legend=True)\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "#Evaluating the errors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE1=mean_squared_error(fit1.fittedvalues, series)\n",
    "MSE2=mean_squared_error(fit2.fittedvalues, series)\n",
    "MSE3=mean_squared_error(fit3.fittedvalues, series)\n",
    "\n",
    "print('Summary of errors resulting from SES models 1, 2 & 3:')\n",
    "import pandas as pd\n",
    "cars = {'Model': ['MSE'],\n",
    "        'LES model 1': [MSE1],\n",
    "        'LES model 2': [MSE2],\n",
    "        'LES model 3': [MSE3]\n",
    "        }\n",
    "AllErrors = pd.DataFrame(cars, columns = ['Model', 'LES model 1', 'LES model 2', 'LES model 3'])\n",
    "print(AllErrors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234ea5b",
   "metadata": {},
   "source": [
    "# Holt-Winter’s Method with Additive Seasonality\n",
    "\n",
    "The equations are:\n",
    "\n",
    "$$L_t = \\alpha(Y_t − S_{t−s} ) + (1 − \\alpha)(L_{t−1} + b_{t−1} )$$\n",
    "$$b_t = \\beta (L_t − L_{t−1} ) + (1 − \\beta )b_{t−1}$$\n",
    "$$S_t = \\gamma(Y_t − L_t ) + (1 − \\gamma)S_{t−s}$$\n",
    "$$F_{t+m} = L_t + b_t m + S_{t−s+m}$$\n",
    "\n",
    "where $s$ is the number of periods in one cycle. The initial values of $L_s$ and $b_s$ can be as in the multiplicative case. The initial seasonal indices can be taken as:\n",
    "\n",
    "$$S_k = Y_k − L_s , k = 1, 2, ..., s.$$\n",
    "\n",
    "Similarly, the parameters $\\alpha$, $\\beta$ , $\\gamma$ should lie in the interval [0, 1], and can again be selected by minimising MAE, MSE or MAPE.\n",
    "\n",
    "Below, cement production data is loaded and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b685b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "series = read_excel('CementProduction.xls', sheet_name='Data', header=0,\n",
    "              index_col=0, parse_dates=True)\n",
    "series.index.freq = 'MS'\n",
    "series  = series['Observations'].iloc[:]\n",
    "\n",
    "# ===================================\n",
    "# Holt-Winter method in different scenarios #\n",
    "# ===================================\n",
    "# ===================================\n",
    "# Model 1: Holt-Winter method with additive trend and seasonality\n",
    "# Here, alpha = 0.3, beta=0.5, gamma=0.7\n",
    "# ===================================\n",
    "fit1 = ExponentialSmoothing(series, seasonal_periods=12, trend='add', seasonal='add').fit(smoothing_level = 0.3, smoothing_trend=0.5,  smoothing_seasonal=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c981fec-67c6-41be-bf4c-8d1d51ff19a0",
   "metadata": {},
   "source": [
    "Following the previous example, complete the cell below to apply the Holt-Winter method with additive trend and multiplicative seasonality. You may want to consult the `statsmodels` <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513da86-193c-47e1-b652-e51f9eab703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Model 2: Holt-Winter method with additive trend and multiplicative seasonality\n",
    "# Here, alpha = 0.3, beta=0.5, gamma=0.7\n",
    "# ===================================\n",
    "fit2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de1e69-da9e-4876-beff-88fac70677c4",
   "metadata": {},
   "source": [
    "Then, read and run the code below for the other cases, and to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da31563-c1e7-4088-9e6d-a60c87e1bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Model 3: Holt-Winter method with additive trend and seasonality\n",
    "# Here, the parameters alpha, beta, and gamma are optimized\n",
    "# ===================================\n",
    "fit3 = ExponentialSmoothing(series, seasonal_periods=12, trend='add', seasonal='add').fit()\n",
    "\n",
    "# ===================================\n",
    "# Model 4: Holt-Winter method with additive trend and multiplicative seasonality\n",
    "# Here, the parameters alpha, beta, and gamma are optimized\n",
    "# ===================================\n",
    "fit4 = ExponentialSmoothing(series, seasonal_periods=12, trend='add', seasonal='mul').fit()\n",
    "\n",
    "fit2.fittedvalues.plot(color='blue')\n",
    "fit1.fittedvalues.plot(color='red')\n",
    "fit3.fittedvalues.plot(color='green')\n",
    "fit4.fittedvalues.plot(color='yellow')\n",
    "\n",
    "print(\"Forecasting Cement Production with Holt-Winters method\")\n",
    "#=====================================\n",
    "# Time and forecast plots\n",
    "#=====================================\n",
    "#series.rename('Time plot of original series')\n",
    "series.plot(color='black',label = 'Time plot of original series', legend=True)\n",
    "fit1.forecast(12).rename('Model 1: HW-additive seasonality').plot(color='red', legend=True)\n",
    "fit2.forecast(12).rename('Model 2: HW-multiplicative seasonality').plot(color='blue', legend=True)\n",
    "fit3.forecast(12).rename('Model 3: Opt HW-additive seasonality').plot(color='green', legend=True)\n",
    "fit4.forecast(12).rename('Model 4: Opt HW-multiplicative seasonality').plot(color='yellow', legend=True)\n",
    "pyplot.xlabel('Dates')\n",
    "pyplot.ylabel('Values')\n",
    "pyplot.title('HW method-based forecasts for cement production')\n",
    "pyplot.show()\n",
    "\n",
    "#====================================\n",
    "# Evaluating the errors\n",
    "#====================================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE1=mean_squared_error(fit1.fittedvalues, series)\n",
    "MSE2=mean_squared_error(fit2.fittedvalues, series)\n",
    "MSE3=mean_squared_error(fit3.fittedvalues, series)\n",
    "MSE4=mean_squared_error(fit4.fittedvalues, series)\n",
    "\n",
    "#=====================================\n",
    "# Printing the paramters and errors for each scenario\n",
    "#=====================================\n",
    "results=pd.DataFrame(index=[r\"alpha\", r\"beta\", r\"gamma\", r\"l0\", \"b0\", \"MSE\"])\n",
    "params = ['smoothing_level', 'smoothing_trend', 'smoothing_seasonal', 'initial_level', 'initial_trend']\n",
    "results[\"HW model 1\"] = [fit1.params[p] for p in params] + [MSE1]\n",
    "results[\"HW model 2\"] = [fit2.params[p] for p in params] + [MSE2]\n",
    "results[\"HW model 3\"] = [fit3.params[p] for p in params] + [MSE3]\n",
    "results[\"HW model 4\"] = [fit4.params[p] for p in params] + [MSE4]\n",
    "print(results)\n",
    "\n",
    "#=====================================\n",
    "# Evaluating and plotting the residual series for each scenario\n",
    "#=====================================\n",
    "residuals1= fit1.fittedvalues - series\n",
    "residuals2= fit2.fittedvalues - series\n",
    "residuals3= fit3.fittedvalues - series\n",
    "residuals4= fit4.fittedvalues - series\n",
    "residuals1.plot(color='red', label=\"residual plot for model 1'\", legend=True)\n",
    "residuals2.plot(color='blue', label='residual plot for model 2', legend=True)\n",
    "residuals3.plot(color='green', label='residual plot for model 3', legend=True)\n",
    "residuals4.plot(color='yellow', label='residual plot for model 4', legend=True)\n",
    "\n",
    "pyplot.title('Residual plots for models 1-4')\n",
    "pyplot.show()\n",
    "\n",
    "#=====================================\n",
    "# ACF plots of the residual series for each scenario\n",
    "#=====================================\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(residuals1, title='Residual ACF for model 1', lags=50)\n",
    "plot_acf(residuals2, title='Residual ACF for model 2', lags=50)\n",
    "plot_acf(residuals3, title='Residual ACF for model 3', lags=50)\n",
    "plot_acf(residuals4, title='Residual ACF for model 4', lags=50)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c90ce6-476a-4266-8141-f2cc05de3c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
