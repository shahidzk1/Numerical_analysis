{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc19851b",
   "metadata": {},
   "source": [
    "# Coût et Gradient Régularisés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160466a6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "Dans ce TP, vous allez :\n",
    "- étendre les précédentes fonctions de coût linéaire et logistique avec un terme de régularisation.\n",
    "- réexécuter l'exemple précédent de surajustement avec un terme de régularisation ajouté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_common import sigmoid\n",
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52e4c0",
   "metadata": {},
   "source": [
    "# Ajout de la régularisation\n",
    "\n",
    "- Coût\n",
    "    - Les fonctions de coût diffèrent significativement entre la régression linéaire et logistique, mais l'ajout de la régularisation aux équations est le même.\n",
    "- Gradient\n",
    "    - Les fonctions de gradient pour la régression linéaire et logistique sont très similaires. Elles diffèrent seulement dans l'implémentation de $f_{wb}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31eb603",
   "metadata": {},
   "source": [
    "## Fonctions de coût avec régularisation\n",
    "### Fonction de coût pour la régression linéaire régularisée\n",
    "\n",
    "L'équation pour la fonction de coût de la régression linéaire régularisée est :\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \n",
    "où :\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ \n",
    "\n",
    "\n",
    "Comparez cela à la fonction de coût sans régularisation (que vous avez mise en œuvre dans un laboratoire précédent), qui est de la forme :\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "\n",
    "La différence est le terme de régularisation,  <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "    \n",
    "L'inclusion de ce terme encourage la descente de gradient à minimiser la taille des paramètres. Notez que, dans cet exemple, le paramètre $b$ n'est pas régularisé. C'est une pratique standard.\n",
    "\n",
    "Ci-dessous se trouve une implémentation des équations (1) et (2). Notez que cela utilise un *schéma standard pour ce cours*, une boucle `for` sur tous les exemples `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7536c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcule le coût sur tous les exemples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,)): valeurs cibles\n",
    "      w (ndarray (n,)): paramètres du modèle  \n",
    "      b (scalaire)      : paramètre du modèle\n",
    "      lambda_ (scalaire): Contrôle le montant de la régularisation\n",
    "    Returns:\n",
    "      cout_total (scalaire):  coût \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cout = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalaire, voir np.dot\n",
    "        cout = cout + (f_wb_i - y[i])**2                               #scalaire             \n",
    "    cout = cout / (2 * m)                                              #scalaire  \n",
    " \n",
    "    cout_reg = 0\n",
    "    for j in range(n):\n",
    "        cout_reg += (w[j]**2)                                          #scalaire\n",
    "    cout_reg = (lambda_/(2*m)) * cout_reg                              #scalaire\n",
    "    \n",
    "    cout_total = cout + cout_reg                                       #scalaire\n",
    "    return cout_total                                                  #scalaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excécutez la fonction ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Coût régularisé:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Coût régularisé: </b> 0.07917239320214275 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935677c4",
   "metadata": {},
   "source": [
    "### Fonction de coût pour la régression logistique régularisée\n",
    "Pour la régression logistique régularisée, la fonction de coût est de la forme\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\n",
    "où:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$ \n",
    "\n",
    "Comparez cela à la fonction de coût sans régularisation (que vous avez implémentée dans un TP précédent):\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n",
    "\n",
    "Comme c'était le cas dans la régression linéaire ci-dessus, la différence est le terme de régularisation, qui est    <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "\n",
    "Inclure ce terme encourage la descente de gradient à minimiser la taille des paramètres. Notez que, dans cet exemple, le paramètre $b$ n'est pas régularisé. C'est une pratique standard. \n",
    "\n",
    "Veuillez implémenter la fonction de coût pour la régression logistique régularisée dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcule le coût sur tous les exemples\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,)): valeurs cibles\n",
    "      w (ndarray (n,)): paramètres du modèle  \n",
    "      b (scalaire)    : paramètre du modèle\n",
    "      lambda_ (scalaire): Contrôle la quantité de régularisation\n",
    "    Returns:\n",
    "      total_cost (scalaire): coût \n",
    "    \"\"\"\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Coût régularisé:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Coût régularisé: </b> 0.6850849138741673 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8c24b",
   "metadata": {},
   "source": [
    "## Descente de gradient avec régularisation\n",
    "L'algorithme de base pour exécuter la descente de gradient ne change pas avec la régularisation, il est :\n",
    "$$\\begin{align*}\n",
    "&\\text{répéter jusqu'à convergence :} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{pour j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "Où chaque itération effectue des mises à jour simultanées sur $w_j$ pour tous les $j$.\n",
    "\n",
    "Ce qui change avec la régularisation est le calcul des gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26388066",
   "metadata": {},
   "source": [
    "### Calcul du Gradient avec régularisation (pour la régression linéaire et logistique)\n",
    "Le calcul du gradient pour la régression linéaire et logistique est presque identique, la seule différence étant le calcul de $f_{\\mathbf{w}b}$.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m est le nombre d'exemples d'entraînement dans l'ensemble de données      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ est la prédiction du modèle, tandis que $y^{(i)}$ est la cible\n",
    "\n",
    "      \n",
    "* Pour un modèle de régression <span style=\"color:blue\"> **linéaire** </span>  \n",
    "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "* Pour un modèle de régression <span style=\"color:blue\"> **logistique** </span>  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    où $g(z)$ est la fonction sigmoïde:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n",
    "Le terme qui ajoute la régularisation est le <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb7cb6",
   "metadata": {},
   "source": [
    "### Fonction de gradient pour la régression linéaire régularisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Calcule le gradient pour la régression linéaire\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,)): valeurs cibles\n",
    "      w (ndarray (n,)): paramètres du modèle  \n",
    "      b (scalaire)    : paramètre du modèle\n",
    "      lambda_ (scalaire): Contrôle la quantité de régularisation\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): Le gradient du coût par rapport aux paramètres w. \n",
    "      dj_db (scalaire):       Le gradient du coût par rapport au paramètre b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           # (nombre d'exemples, nombre de caractéristiques)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la cellule suivante pour tester cette fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"dj_dw régularisé:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29ccac",
   "metadata": {},
   "source": [
    "**Résultat attendu**\n",
    "```\n",
    "dj_db: 0.6648774569425726\n",
    "dj_dw régularisé:\n",
    " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7e795",
   "metadata": {},
   "source": [
    "### Fonction de gradient pour la régression logistique régularisée\n",
    "\n",
    "En suivant l'exemple de la fonction de gradient pour la régression linéaire régularisée, implémentez-la pour la régression logistique dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b59f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Calcule le gradient pour la régression logistique\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,)): valeurs cibles\n",
    "      w (ndarray (n,)): paramètres du modèle  \n",
    "      b (scalaire)    : paramètre du modèle\n",
    "      lambda_ (scalaire): Contrôle la quantité de régularisation\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): Le gradient du coût par rapport aux paramètres w. \n",
    "      dj_db (scalaire)            : Le gradient du coût par rapport au paramètre b. \n",
    "    \"\"\"\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la cellule suivante pour tester cette fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"dj_dw régularisé:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résultat attendu**\n",
    "```\n",
    "dj_db: 0.341798994972791\n",
    "dj_dw régularisé:\n",
    " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4007c65",
   "metadata": {},
   "source": [
    "## Réexécutez l'exemple de surapprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "display(output)\n",
    "ofit = overfit_example(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec0136",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessus, essayez la régularisation sur l'exemple précédent. En particulier :\n",
    "- Catégorie (régression logistique)\n",
    "    - définissez le degré à 6, lambda à 0 (pas de régularisation), ajustez les données\n",
    "    - maintenant définissez lambda à 1 (augmentez la régularisation), ajustez les données, remarquez la différence.\n",
    "- Régression (régression linéaire)\n",
    "    - essayez la même procédure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75169790",
   "metadata": {},
   "source": [
    "## Félicitations !\n",
    "Vous avez :\n",
    "- des exemples de fonction de coût et de gradient avec régularisation ajoutée pour la régression linéaire et logistique\n",
    "- développé une certaine intuition sur la façon dont la régularisation peut réduire le surapprentissage. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
