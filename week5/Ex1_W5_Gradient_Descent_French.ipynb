{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7563e209fd92fad9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Descente de gradient pour la régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc0692cdc4a0f3",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "Dans ce TP, vous allez :\n",
    "- mettre à jour la descente de gradient pour la régression logistique.\n",
    "- explorer la descente de gradient sur un ensemble de données que vous connaissez déjà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfba3abb10e1b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T08:43:02.056528Z",
     "start_time": "2024-03-19T08:43:01.662280Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
    "from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925b702f5dde7d2",
   "metadata": {},
   "source": [
    "## Ensemble de données\n",
    "Commençons par le même ensemble de données à deux caractéristiques utilisé dans le TP de la frontière de décision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa659a851b5fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T08:43:04.368050Z",
     "start_time": "2024-03-19T08:43:04.365790Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ac76ec0f9cab0",
   "metadata": {},
   "source": [
    "Comme précédemment, nous utiliserons une fonction externe pour tracer ces données. \n",
    "Les points de données avec l'étiquette $y=1$ sont représentés par des croix rouges, tandis que les points de données avec l'étiquette $y=0$ sont représentés par des cercles bleus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12326a66e927b10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T08:43:14.297526Z",
     "start_time": "2024-03-19T08:43:14.198661Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e91edb42c0029",
   "metadata": {},
   "source": [
    "## Descente de gradient logistique\n",
    "\n",
    "Rappelez-vous que l'algorithme de descente de gradient utilise le calcul du gradient :\n",
    "$$\\begin{align*}\n",
    "&\\text{répéter jusqu'à convergence :} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{pour j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "Où chaque itération effectue des mises à jour simultanées sur $w_j$ pour tous les $j$, où\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m est le nombre d'exemples d'entraînement dans l'ensemble de données      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ est la prédiction du modèle, tandis que $y^{(i)}$ est la cible\n",
    "* Pour un modèle de régression logistique  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    où $g(z)$ est la fonction sigmoïde :  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4434aa05b7307e",
   "metadata": {},
   "source": [
    "### Implémentation de la Descente de Gradient\n",
    "L'implémentation de l'algorithme de descente de gradient a deux composants :\n",
    "- La boucle utlisant l'équation (1) ci-dessus. C'est `gradient_descent` et elle vous est généralement fournie dans les TPs optionnels et pratiques.\n",
    "- Le calcul du gradient actuel, équations (2,3). C'est la fonction `compute_gradient_logistic`.\n",
    "- On vous demandera de l'implémenter dans le TP pratique de cette semaine.\n",
    "\n",
    "#### Calcul du Gradient, Description du Code\n",
    "Implémentez l'équation (2),(3) ci-dessus pour tous les $w_j$ et $b$.\n",
    "Il existe de nombreuses façons de le faire, mais voici une proposition : \n",
    "- initialiser les variables pour accumuler `dj_dw` et `dj_db`\n",
    "- pour chaque exemple\n",
    "    - calculer l'erreur pour cet exemple $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n",
    "    - pour chaque valeur d'entrée $x_{j}^{(i)}$ dans cet exemple,\n",
    "        - multiplier l'erreur par l'entrée $x_{j}^{(i)}$, et ajouter à l'élément correspondant de `dj_dw`. (équation 2 ci-dessus)\n",
    "    - ajouter l'erreur à `dj_db` (équation 3 ci-dessus)\n",
    "\n",
    "- diviser `dj_db` et `dj_dw` par le nombre total d'exemples (m)\n",
    "- notez que $\\mathbf{x}^{(i)}$ avec numpy est `X[i,:]` ou `X[i]` et que $x_{j}^{(i)}$ est `X[i,j]`\n",
    "\n",
    "Veuillez l'implémenter dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5e181d2fcaeb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:50:24.912004Z",
     "start_time": "2024-03-19T01:50:24.908708Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcule le gradient pour la régression logistique\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,)): valeurs cibles\n",
    "      w (ndarray (n,)): paramètres du modèle  \n",
    "      b (scalaire)    : paramètre du modèle\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): Le gradient du coût par rapport aux paramètres w. \n",
    "      dj_db (scalaire)    : Le gradient du coût par rapport au paramètre b. \n",
    "    \"\"\"\n",
    "        \n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32e2c2a8475906",
   "metadata": {},
   "source": [
    "Vérifiez son implémentation avec la celle ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d267f9084760c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:27.070007Z",
     "start_time": "2024-03-19T01:40:26.980666Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b58b6cd6d0e48",
   "metadata": {},
   "source": [
    "**Résultat attendu**\n",
    "``` \n",
    "dj_db: 0.49861806546328574\n",
    "dj_dw: [0.498333393278696, 0.49883942983996693]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3910b16",
   "metadata": {},
   "source": [
    "#### Code de la Descente de Gradient\n",
    "Le code mettant en œuvre l'équation (1) ci-dessus est implémenté juste après. Prenez un moment pour localiser et comparer les fonctions utilisée dans cette fonction  avec les équations ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f76b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Effectue la descente de gradient en batch\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Données, m exemples avec n caractéristiques\n",
    "      y (ndarray (m,))    : valeurs cibles\n",
    "      w_in (ndarray (n,)) : Valeurs initiales des paramètres du modèle  \n",
    "      b_in (scalaire)     : Valeur initiale du paramètre du modèle\n",
    "      alpha (float)       : Taux d'apprentissage\n",
    "      num_iters (scalaire): nombre d'itérations pour exécuter la descente de gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))    : Valeurs mises à jour des paramètres\n",
    "      b (scalaire)        : Valeur mise à jour du paramètre \n",
    "    \"\"\"\n",
    "    # Un tableau pour stocker le coût J et les w à chaque itération principalement pour le graphique plus tard\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #éviter de modifier le w global dans la fonction\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcule le gradient et met à jour les paramètres\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Met à jour les paramètres en utilisant w, b, alpha et le gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Enregistre le coût J à chaque itération\n",
    "        if i<100000:      # prévenir l'épuisement des ressources \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Imprime le coût à des intervalles de 10 fois ou autant d'itérations si < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Coût {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #retourne le w, b final et l'historique de J pour le graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5699c1",
   "metadata": {},
   "source": [
    "Exécutons la descente de gradient sur notre ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8442b78357641f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:30.559041Z",
     "start_time": "2024-03-19T01:40:30.513702Z"
    }
   },
   "outputs": [],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\Nouveaux paramètres: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204c3615f5c4de3",
   "metadata": {},
   "source": [
    "#### Graphique du résultat de la descente de gradient : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92309b574a278d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:31.276552Z",
     "start_time": "2024-03-19T01:40:31.223001Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "# affiche la probabilité\n",
    "plt_prob(ax, w_out, b_out)\n",
    "\n",
    "# donnée originales \n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')   \n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "plot_data(X_train,y_train,ax)\n",
    "\n",
    "# frontière de décision\n",
    "x0 = -b_out/w_out[0]\n",
    "x1 = -b_out/w_out[1]\n",
    "ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2933b6",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessus :\n",
    " - l'ombrage reflète la probabilité y=1 (résultat avant la frontière de décision)\n",
    " - la frontière de décision est la ligne où la probabilité = 0,5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859f8a7",
   "metadata": {},
   "source": [
    "## Un autre ensemble de données\n",
    "Revenons à un ensemble de données à une variable. Avec seulement deux paramètres, $w$, $b$, il est possible de tracer la fonction de coût en utilisant un graphique de contour pour avoir une meilleure idée de ce que fait la descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007bd95f04eb434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:32.002763Z",
     "start_time": "2024-03-19T01:40:32.000239Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a4090",
   "metadata": {},
   "source": [
    "Comme précédemment, nous utiliserons une fonction externe pour tracer ces données. Les points de données avec l'étiquette $y=1$ sont représentés par des croix rouges, tandis que les points de données avec l'étiquette $y=0$ sont représentés par des cercles bleus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500df0322401af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:32.434208Z",
     "start_time": "2024-03-19T01:40:32.392951Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "plt_tumor_data(x_train, y_train, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ba96b",
   "metadata": {},
   "source": [
    "Dans le graphique ci-dessous, essayez :\n",
    "- de changer $w$ et $b$ en cliquant à l'intérieur du graphique de contour en haut à droite.\n",
    "    - les changements peuvent prendre une ou deux secondes\n",
    "    - notez la valeur changeante du coût sur le graphique en haut à gauche.\n",
    "    - notez que le coût est accumulé par une perte sur chaque exemple (lignes verticales pointillées)\n",
    "- exécutez la descente de gradient en cliquant sur le bouton orange.\n",
    "    - notez le coût qui diminue régulièrement (le contour et le graphique du coût sont en log(coût)\n",
    "    - cliquer dans le graphique de contour réinitialisera le modèle pour une nouvelle exécution\n",
    "- pour réinitialiser le graphique, réexécutez la cellule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30130c681f107e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T01:40:33.216314Z",
     "start_time": "2024-03-19T01:40:32.775874Z"
    }
   },
   "outputs": [],
   "source": [
    "w_range = np.array([-1, 7])\n",
    "b_range = np.array([1, -14])\n",
    "quad = plt_quad_logistic( x_train, y_train, w_range, b_range )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf60bd",
   "metadata": {},
   "source": [
    "## Félicitations !\n",
    "Vous avez :\n",
    "- examiné les formules et l'implémentation du calcul du gradient pour la régression logistique\n",
    "- utilisé ces routines dans\n",
    "    - l'exploration d'un ensemble de données à une seule variable\n",
    "    - l'exploration d'un ensemble de données à deux variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
