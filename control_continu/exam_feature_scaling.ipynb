{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b380c01",
   "metadata": {},
   "source": [
    "# Exam goal\n",
    "This lab simulates time and momentum data with the goal of predicting force. Force, defined as ($F = 5 \\times t + 0.002 * p$), is added as a label. Random noise has been added as well to the force values so that it mimics a real world scenario. Assume you don't know a-priory the proportionality constants that from momentum and time give you the force in this specific case, and use linear regression to predict the force from the available data.\n",
    "\n",
    "The goal of the exercise is to show that feature scaling help in improving the performance of a regression model. Since in the course you have been exposed to z score normalization, you should use that to show that a model's prediction on the test data improves if the model is trained and tested on normalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e456264",
   "metadata": {},
   "source": [
    "In the cell below, the dataset is generated and then split into separate training and testing samples, which you have to use through the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e604dfe1-ce48-4de3-b10a-c4d467b4bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for time, momentum, and force\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "time = np.random.uniform(0, 1, num_samples)\n",
    "momentum = np.random.uniform(0, 1000, num_samples)\n",
    "force = 5 * time + 0.002 * momentum + np.random.normal(0, 1, num_samples)\n",
    "\n",
    "# Split the dataset into training (X_train, y_train) and testing (X_test, y_test) sets\n",
    "X_ = np.column_stack((time, momentum))\n",
    "y_ = force\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8f797",
   "metadata": {},
   "source": [
    "In the cell below, implement the cost function for linear regression, including a regularization term lambda_ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8b98d-d4e4-4b10-b1a0-553ed20f4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(    ):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4ab37",
   "metadata": {},
   "source": [
    "In the cell below, implement the gradient computation for the linear regression model you defined above. You don't need to include a regularization term in the gradient computation, in this simple exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93295df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(    ): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e329b7",
   "metadata": {},
   "source": [
    "In the cell below, the gradient descent algorithm is provided to you. Make sure your implementation of the cost and gradient functions is compatible with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeaceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      lambda_ (scalar)   : regularization parameter for the cost function\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient(X, y, w, b )\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      \n",
    "            J_history.append( compute_cost(X, y, w, b, lambda_) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263c081",
   "metadata": {},
   "source": [
    "In the cell below, intitialize the model parameters appropriately and call the gradient descent function on 1000 iterations. The regularization parameter is fixed to lambda_ = 0.1 and given to you, while you have to find a value of the learning rate alpha_ in the interval between $10^{-4}$ and $10^{-8}$ that ensures the gradient descent algorithm converges.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a47f35-6721-4129-b79f-6ee0b626ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing prior to feature scaling\n",
    "w_tmp  = \n",
    "b_tmp  = \n",
    "# find an alpha_ value between 10^-4 and 10^-8 that ensures convergence for the gradient descent\n",
    "alpha_ = \n",
    "# do not need to change lambda_ or iters\n",
    "lambda_ = 0.1\n",
    "iters = 1000\n",
    "\n",
    "# call the gradient descent function here below on the training dataset, using the arguments you have just initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a392dde",
   "metadata": {},
   "source": [
    "Complete the cell below so that \"y_pred\" stores the model predictions for the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34b396-7516-406c-bf41-851710ebe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6796f84",
   "metadata": {},
   "source": [
    "Calculate the cost for the X_test while using lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3addc5",
   "metadata": {},
   "source": [
    "You should get something like 4.057513207508225"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bf88a",
   "metadata": {},
   "source": [
    "In the cell below, define a function that implements the $z$-score normalization for the model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056722e-9ea3-4c75-8abc-30339aae0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(     ):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09197785",
   "metadata": {},
   "source": [
    "In the cell below, use the $z$-score normalization you implemented above to obtain the scaled training (\"X_train_norm\") and testing (\"X_test_norm\") datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = \n",
    "X_test_norm = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003779b9",
   "metadata": {},
   "source": [
    "In the cell below, call the gradient descent function for 1000 iterations using the scaled training (\"X_train_norm\") and testing (\"X_test_norm\") datasets. The regularization parameter is fixed to lambda_ = 0.1 and given to you. **Also the learning rate alpha_ in this case has been changed to alpha_ = 0.01 and given to you**: you don't need to change its value. This is because feature scaling helps the convergence and allows you to use a greater learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c89e54-f9a0-4209-ae12-8adb867f80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing after feature scaling\n",
    "w_tmp  = \n",
    "b_tmp  = \n",
    "# don't need to change alpha_, lambda_, iters\n",
    "alpha_ = 0.01 \n",
    "lambda_ = 0.1\n",
    "iters = 1000\n",
    "\n",
    "# call the gradient descent function here below on the training dataset, using scaled fetures and the arguments you have just initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5a82a",
   "metadata": {},
   "source": [
    "Complete the cell below so that \"y_pred_xnorm\" stores the model predictions for the testing dataset using the **scaled** features you have previously computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08677db1-4ac5-4272-96ea-44a0a98f65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xnorm = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61fd60",
   "metadata": {},
   "source": [
    "Finally, complete the cell below to obtain the MSE for your model after feature scaling. You expect to obtain a value noticeably smaller than what you have previously obtained by training the dataset on the original features, before $z$-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122b0a4",
   "metadata": {},
   "source": [
    "Calculate the cost function again for the X_test_norm while using lambda_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ebfe6",
   "metadata": {},
   "source": [
    "This time the cost should be lower and should be something like 0.949801563674332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377731f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
