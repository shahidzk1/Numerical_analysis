{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise à l'échelle des caractéristiques et taux d'apprentissage (Multi-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "Dans ce TP, vous allez :\n",
    "- Utiliser les fonctions à variables multiples développées dans le TP précédent\n",
    "- Exécuter la descente de gradient sur un ensemble de données avec plusieurs caractéristiques\n",
    "- Explorer l'impact du *taux d'apprentissage alpha* sur la descente de gradient\n",
    "- Améliorer les performances de la descente de gradient par la *mise à l'échelle des caractéristiques* en utilisant la normalisation du score z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outils\n",
    "Vous utiliserez les fonctions développées dans le dernier TP ainsi que matplotlib et NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dcf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import  load_house_data, run_gradient_descent \n",
    "from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\n",
    "from lab_utils_common import dlc\n",
    "np.set_printoptions(precision=2)\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "|Notation générale <br />  | Description| Python (si applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalaire, en non gras                                                      ||\n",
    "| $\\mathbf{a}$ | vecteur, en gras                                                 ||\n",
    "| $\\mathbf{A}$ | matrice, majuscule en gras                                         ||\n",
    "| **Régression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | matrice des exemples d'entraînement                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | cibles des exemples d'entraînement                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{ème}$ exemple d'entraînement | `X[i]`, `y[i]`|\n",
    "| m | nombre d'exemples d'entraînement | `m`|\n",
    "| n | nombre de caractéristiques dans chaque exemple | `n`|\n",
    "|  $\\mathbf{w}$  |  paramètre : poids,                       | `w`    |\n",
    "|  $b$           |  paramètre : biais                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | Le résultat de l'évaluation du modèle à  $\\mathbf{x}^{(i)}$ paramétré par $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| le gradient ou dérivée partielle du coût par rapport à un paramètre $w_j$ |`dj_dw[j]`| \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| le gradient ou dérivée partielle du coût par rapport à un paramètre $b$| `dj_db`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Énoncé du problème\n",
    "\n",
    "Comme dans les TPs précédents, vous utiliserez toujours l'exemple de la prédiction du prix des logements. L'ensemble de données d'entraînement contient de nombreux exemples avec 4 caractéristiques (taille, chambres, étages et âge) montrés dans le tableau ci-dessous. Notez que, dans ce TP, la caractéristique Taille est en pieds carrés alors que les TPs précédents utilisaient le millier de pieds carrés. Cet ensemble de données est plus grand que celui du TP précédent.\n",
    "\n",
    "Nous aimerions construire un modèle de régression linéaire en utilisant ces valeurs afin que nous puissions ensuite prédire le prix pour d'autres maisons - disons, une maison de 1200 pieds carrés, 3 chambres, 1 étage, 40 ans.\n",
    "\n",
    "## Ensemble de données : \n",
    "| Taille (pieds carrés) | Nombre de chambres  | Nombre d'étages | Âge de la maison | Prix (milliers de dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|----------------------- |  \n",
    "| 952             | 2                   | 1                | 65           | 271.5                  |  \n",
    "| 1244            | 3                   | 2                | 64           | 232                    |  \n",
    "| 1947            | 3                   | 2                | 17           | 509.8                  |  \n",
    "| ...             | ...                 | ...              | ...          | ...                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7376783f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_house_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# charger l'ensemble de données\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_house_data\u001b[49m()\n\u001b[1;32m      3\u001b[0m X_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaille(pieds carrés)\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchambres\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124métages\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mâge\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_house_data' is not defined"
     ]
    }
   ],
   "source": [
    "# charger l'ensemble de données\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['taille(pieds carrés)','chambres','étages','âge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the dataset and its features by plotting each feature versus price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdf9426",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig,ax\u001b[38;5;241m=\u001b[39m\u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m), sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax)):\n\u001b[1;32m      3\u001b[0m     ax[i]\u001b[38;5;241m.\u001b[39mscatter(X_train[:,i],y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Prix (milliers de $)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La représentation graphique de chaque caractéristique par rapport à la cible, le prix, donne une indication de quelles caractéristiques ont la plus forte influence sur le prix. Ci-dessus, l'augmentation de la taille augmente également le prix. Les chambres et les étages ne semblent pas avoir un fort impact sur le prix. Les maisons plus récentes ont des prix plus élevés que les maisons plus anciennes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "## Descente de gradient avec plusieurs variables\n",
    "Voici les équations que vous avez développées dans le dernier TP sur la descente de gradient pour plusieurs variables :\n",
    "\n",
    "$$\\begin{align*} \\text{répéter}&\\text{ jusqu'à convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{pour j = 0..n-1}\\newline\n",
    "&b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "où, n est le nombre de caractéristiques, les paramètres $w_j$,  $b$, sont mis à jour simultanément et où  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "* m est le nombre d'exemples d'entraînement dans l'ensemble de données\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ est la prédiction du modèle, tandis que $y^{(i)}$ est la valeur cible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taux d'apprentissage\n",
    "\n",
    "Le cours a porté sur certaines questions liées à la définition du taux d'apprentissage $\\alpha$. Le taux d'apprentissage contrôle la taille de la mise à jour des paramètres. Voir l'équation (1) ci-dessus. Il est commun à tous les paramètres.\n",
    "\n",
    "Exécutons la descente de gradient et essayons quelques paramétrages de $\\alpha$ sur notre ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\alpha$ = 9.9e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a193f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_gradient_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#On initialise alpha à 9.9e-7\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _, _, hist \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gradient_descent\u001b[49m(X_train, y_train, \u001b[38;5;241m10\u001b[39m, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9.9e-7\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_gradient_descent' is not defined"
     ]
    }
   ],
   "source": [
    "#On initialise alpha à 9.9e-7\n",
    "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the learning rate is too high.  The solution does not converge. Cost is *increasing* rather than decreasing. Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d64acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique à droite montre la valeur de l'un des paramètres, $w_0$. À chaque itération, il dépasse la valeur optimale et, par conséquent, le coût finit par augmenter plutôt que de se rapprocher du minimum. Notez que ce n'est pas une image complètement précise car il y a 4 paramètres qui sont modifiés à chaque passage plutôt qu'un seul. Ce graphique ne montre que $w_0$ avec les autres paramètres fixés à des valeurs très petites. Dans ce graphique et les suivants, vous pouvez remarquer que les lignes bleue et orange sont légèrement décalées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $\\alpha$ = 9e-7\n",
    "Essayons une valeur un peu plus petite et voyons ce qui se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7798585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise alpha à 9e-7\n",
    "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coût diminue au fur et à mesure, montrant que alpha n'est pas trop grand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train, y_train, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur la gauche, vous voyez que le coût diminue comme il se doit. Sur la droite, vous pouvez voir que $w_0$ oscille toujours autour du minimum, mais il diminue à chaque itération plutôt qu'augmenter. Notez ci-dessus que `dj_dw[0]` change de signe à chaque itération lorsque `w[0]` passe au dessus la valeur optimale.\n",
    "Cette valeur alpha convergera. Vous pouvez varier le nombre d'itérations pour voir comment elle se comporte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\alpha$ = 1e-7\n",
    "Essayons une valeur plus petite d'alpha et voyons ce qui se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on initialise alpha à 1e-7\n",
    "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le coût diminue au fur et à mesure, montrant que alpha n'est pas trop grand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_i_w(X_train,y_train,hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur la gauche, vous voyez que le coût diminue comme il se doit. Sur la droite, vous pouvez voir que $w_0$ diminue sans traverser le minimum. Notez ci-dessus que `dj_w0` est négatif tout au long de l'exécution. Cette solution convergera également, bien que pas tout à fait aussi rapidement que l'exemple précédent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mise à l'échelle des caractéristiques\n",
    "\n",
    "Pendant les cours, l'importance de la remise à l'échelle de l'ensemble de données afin que les caractéristiques aient une plage similaire a été mentionnée.\n",
    "Si vous êtes intéressé par les détails de pourquoi c'est le cas, cliquez sur l'en-tête 'détails' ci-dessous. Sinon, la section ci-dessous vous guidera à travers une mise en œuvre de comment faire la mise à l'échelle des caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Détails</b></font>\n",
    "</summary>\n",
    "\n",
    "Revenons à la situation avec $\\alpha$ = 9e-7. C'est assez proche de la valeur maximale que nous pouvons définir pour $\\alpha$ sans diverger. Voici une courte exécution montrant les premières itérations :\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_ShortRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "\n",
    "Ci-dessus, bien que le coût soit en train de diminuer, il est clair que $w_0$ progresse plus rapidement que les autres paramètres en raison de son gradient beaucoup plus grand.\n",
    "\n",
    "Le graphique ci-dessous montre le résultat d'une très longue exécution avec $\\alpha$ = 9e-7. Cela prend plusieurs heures.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./images/C1_W2_Lab06_LongRun.PNG\" style=\"width:1200px;\" >\n",
    "</figure>\n",
    "    \n",
    "Ci-dessus, vous pouvez voir que le coût a diminué lentement après sa réduction initiale. Remarquez la différence entre `w0` et `w1`,`w2`,`w3` ainsi que `dj_dw0` et `dj_dw1-3`. `w0` atteint sa valeur finale presque très rapidement et `dj_dw0` a rapidement diminué à une petite valeur montrant que `w0` est proche de la valeur finale. Les autres paramètres ont été réduits beaucoup plus lentement.\n",
    "\n",
    "Pourquoi cela ? Y a-t-il quelque chose que nous pouvons améliorer ?\n",
    "<figure>\n",
    "    <center> <img src=\"./images/C1_W2_Lab06_scale.PNG\"   ></center>\n",
    "</figure>   \n",
    "\n",
    "La figure ci-dessus montre pourquoi les $w$ sont mis à jour de manière inégale. \n",
    "- $\\alpha$ est partagé par toutes les mises à jour de paramètres ($w$ et $b$).\n",
    "- le terme d'erreur commun est multiplié par les caractéristiques pour les $w$. (pas $b$).\n",
    "- les caractéristiques varient significativement en magnitude, ce qui fait que certaines caractéristiques se mettent à jour beaucoup plus rapidement que d'autres. Dans ce cas, $w_0$ est multiplié par 'taille(pieds carrés)', qui est généralement > 1000, tandis que $w_1$ est multiplié par 'nombre de chambres', qui est généralement de 2-4. \n",
    "    \n",
    "La solution est la mise à l'échelle des caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cours, trois techniques différentes ont été discutées : \n",
    "- La mise à l'échelle des caractéristiques, qui consiste essentiellement à diviser chaque caractéristique positive par sa valeur maximale, ou plus généralement, à redimensionner chaque caractéristique par ses valeurs minimale et maximale en utilisant (x-min)/(max-min). Les deux méthodes normalisent les caractéristiques à l'intervalle de -1 et 1, la première méthode fonctionnant pour les caractéristiques positives, ce qui est simple et convient bien pour l'exemple de la conférence, et la seconde méthode fonctionnant pour toutes les caractéristiques.\n",
    "- Normalisation de la moyenne : $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n",
    "- Normalisation du score Z que nous explorerons ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation par le score z\n",
    "Après la normalisation par le score z, toutes les caractéristiques auront une moyenne de 0 et un écart type de 1.\n",
    "\n",
    "Pour mettre en œuvre la normalisation par le score z, ajustez vos valeurs d'entrée comme le montre cette formule :\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \n",
    "où $j$ sélectionne une caractéristique ou une colonne dans la matrice $\\mathbf{X}$. $µ_j$ est la moyenne de toutes les valeurs pour la caractéristique (j) et $\\sigma_j$ est l'écart type de la caractéristique (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Note d'implémentation :** Lors de la normalisation des caractéristiques, il est important\n",
    "de stocker les valeurs utilisées pour la normalisation - la valeur moyenne et l'écart type utilisés pour les calculs. Après avoir appris les paramètres\n",
    "du modèle, nous voulons souvent prédire les prix des maisons que nous n'avons pas\n",
    "vu auparavant. Étant donné une nouvelle valeur x (surface du salon et nombre de chambres),\n",
    "nous devons d'abord normaliser x en utilisant la moyenne et l'écart type\n",
    "que nous avions précédemment calculé à partir de l'ensemble d'entraînement.\n",
    "\n",
    "**Implémentation**\n",
    "Dans la cellule ci-dessous, implémentez la fonction \"zscore_normalize_features\" en suivant les formules ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bba3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    calcule X, normalisé par le score z par colonne\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : données d'entrée, m exemples, n caractéristiques\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): entrée normalisée par colonne\n",
    "      mu (ndarray (n,))     : moyenne de chaque caractéristique\n",
    "      sigma (ndarray (n,))  : écart type de chaque caractéristique\n",
    "    \"\"\"\n",
    "\n",
    "# INDICE\n",
    "# si vous avez implémenté la fonction correctement, après l'avoir exécutée \n",
    "# comme \"X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\" vous devriez obtenir :\n",
    "# X_mu = [1.42e+03 2.72e+00 1.38e+00 3.84e+01], \n",
    "# X_sigma = [411.62   0.65   0.49  25.78]\n",
    "    mu = np.mean(X, axis =0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X-mu) / sigma\n",
    "    return (X_norm, mu, sigma)\n",
    " \n",
    "# vérifions notre travail\n",
    "# from sklearn.preprocessing import scale\n",
    "# scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons les étapes impliquées dans la normalisation par le score Z. Le graphique ci-dessous montre la transformation étape par étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa6d3a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mu     \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(X_train,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \n\u001b[1;32m      2\u001b[0m sigma  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m      3\u001b[0m X_mean \u001b[38;5;241m=\u001b[39m (X_train \u001b[38;5;241m-\u001b[39m mu)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"non normalisé\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Normalisé par le score Z\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution des caractéristiques avant, pendant, après normalisation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique ci-dessus montre la relation entre deux des paramètres de l'ensemble d'entraînement, \"age\" et \"taille (pied carrés)\". *Ces derniers sont tracés à l'échelle égale*. \n",
    "- À gauche : Non normalisé : La plage de valeurs ou la variance de la caractéristique 'size(sqft)' est beaucoup plus grande que celle de l'âge\n",
    "- Au milieu : La première étape supprime la valeur moyenne ou moyenne de chaque caractéristique. Cela laisse des caractéristiques qui sont centrées autour de zéro. Il est difficile de voir la différence pour la caractéristique 'âge', mais 'size(sqft)' est clairement autour de zéro.\n",
    "- À droite : La deuxième étape divise par l'écart type. Cela laisse les deux caractéristiques centrées à zéro avec une échelle similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisons les données et comparons-les aux données originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e12d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# On normalise les données originales\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_norm, X_mu, X_sigma \u001b[38;5;241m=\u001b[39m zscore_normalize_features(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_mu = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_mu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mX_sigma = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_sigma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlage de valeurs par colonne dans X original        :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mptp(X_train,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# On normalise les données originales\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Plage de valeurs par colonne dans X original : {np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Plage de valeurs par colonne dans X normalisé :{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plage de valeurs maximale de chaque colonne est réduite d'un facteur de milliers à un facteur de 2-3 par la normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d207aec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig,ax\u001b[38;5;241m=\u001b[39m\u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax)):\n\u001b[1;32m      3\u001b[0m     norm_plot(ax[i],X_train[:,i],)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_train[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"compte\");\n",
    "fig.suptitle(\"distribution des caractéristiques avant la normalisation\")\n",
    "plt.show()\n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_norm[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"compte\"); \n",
    "fig.suptitle(\"distribution des caractéristiques après la normalisation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez, ci-dessus, que la plage des données normalisées (axe des x) est centrée autour de zéro et est approximativement de +/- 2. Le plus important, c'est que la plage est similaire pour chaque caractéristique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82ccdf",
   "metadata": {},
   "source": [
    "Relançons notre algorithme de descente de gradient avec des données normalisées.\n",
    "Notez la **valeur beaucoup plus grande d'alpha**. Cela accélérera la descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f85b05",
   "metadata": {},
   "source": [
    "Les caractéristiques mises à l'échelle obtiennent des résultats très précis **beaucoup, beaucoup plus rapidement !**. Remarquez que le gradient de chaque paramètre est minuscule à la fin de cette exécution assez courte. Un taux d'apprentissage de 0,1 est un bon départ pour la régression avec des caractéristiques normalisées.\n",
    "Traçons nos prédictions par rapport aux valeurs cibles. Notez que la prédiction est faite en utilisant la caractéristique normalisée tandis que le graphique est montré en utilisant les valeurs de caractéristiques originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df5e9df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prédire la cible en utilisant des caractéristiques normalisées\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mX_norm\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m yp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Prédire la cible en utilisant des caractéristiques normalisées\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "\n",
    "# Tracer les prédictions et les cibles par rapport aux caractéristiques originales    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'cible')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'prédiction')\n",
    "ax[0].set_ylabel(\"Prix\"); ax[0].legend();\n",
    "fig.suptitle(\"cible versus prédiction en utilisant le modèle normalisé par le score Z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e167ad",
   "metadata": {},
   "source": [
    "Les résultats semblent bons. Quelques points à noter :\n",
    "- avec plusieurs caractéristiques, nous ne pouvons plus avoir un seul graphique montrant les résultats par rapport aux caractéristiques.\n",
    "- lors de la génération du graphique, les caractéristiques normalisées ont été utilisées. Toute prédiction utilisant les paramètres appris à partir d'un ensemble d'entraînement normalisé doit également être normalisée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc98bd5",
   "metadata": {},
   "source": [
    "**Prédiction**\n",
    "L'objectif de générer notre modèle est de l'utiliser pour prédire les prix des logements qui ne sont pas dans l'ensemble de données. Prédisons le prix d'une maison de 1200 pieds carrés, 3 chambres, 1 étage, 40 ans. Rappelez-vous, vous devez normaliser les données avec la moyenne et l'écart type dérivés lorsque les données d'entraînement ont été normalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac8bc3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tout d'abord, normalisons notre exemple.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_house \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1200\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m])\n\u001b[1;32m      3\u001b[0m x_house_norm \u001b[38;5;241m=\u001b[39m (x_house \u001b[38;5;241m-\u001b[39m X_mu) \u001b[38;5;241m/\u001b[39m X_sigma\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_house_norm)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Tout d'abord, normalisons notre exemple.\n",
    "x_house = np.array([1200, 3, 1, 40])\n",
    "x_house_norm = (x_house - X_mu) / X_sigma\n",
    "print(x_house_norm)\n",
    "x_house_predict = np.dot(x_house_norm, w_norm) + b_norm\n",
    "print(f\" prix prédit d'une maison de 1200 pieds carrés, 3 chambres, 1 étage, 40 ans = ${x_house_predict*1000:0.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ba02a",
   "metadata": {},
   "source": [
    "**Contours de coût**\n",
    "\n",
    "Une autre façon de voir la mise à l'échelle des caractéristiques est en termes de contours de coût. Lorsque les échelles des caractéristiques ne correspondent pas, le tracé du coût par rapport aux paramètres dans un tracé de contour est asymétrique.\n",
    "\n",
    "Dans le tracé ci-dessous, l'échelle des paramètres est adaptée. Le tracé de gauche est le tracé de contour de coût de w[0], les pieds carrés par rapport à w[1], le nombre de chambres avant la normalisation des caractéristiques. Le tracé est si asymétrique que les courbes complétant les contours ne sont pas visibles. En revanche, lorsque les caractéristiques sont normalisées, le contour de coût est beaucoup plus symétrique. Le résultat est que les mises à jour des paramètres lors de la descente de gradient peuvent faire des progrès égaux pour chaque paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_equal_scale(X_train, X_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you:\n",
    "- utilized the routines for linear regression with multiple features you developed in previous labs\n",
    "- explored the impact of the learning rate  $\\alpha$ on convergence \n",
    "- discovered the value of feature scaling using z-score normalization in speeding convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa03b6",
   "metadata": {},
   "source": [
    "## Félicitations !\n",
    "Dans ce TP, vous avez :\n",
    "- utilisé les fonctions pour la régression linéaire avec plusieurs caractéristiques que vous avez développées dans les TPs précédents\n",
    "- exploré l'impact du taux d'apprentissage $\\alpha$ sur la convergence\n",
    "- découvert l'intérêt de la mise à l'échelle des caractéristiques en utilisant la normalisation par le score z pour accélérer la convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e393368",
   "metadata": {},
   "source": [
    "## Remerciements\n",
    "Les données sur le logement proviennent de l'[ensemble de données sur le logement d'Ames](http://jse.amstat.org/v19n3/decock.pdf) compilé par Dean De Cock pour être utilisé dans l'éducation en science des données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
