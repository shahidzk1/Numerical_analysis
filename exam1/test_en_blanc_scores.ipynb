{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dd4b91-7975-40b6-b5e3-d0f33522abf8",
   "metadata": {},
   "source": [
    "# Projectile Range\n",
    "Suppose we have a projectile with which we want to hit some range. The formula for the range of a projectile is $R = \\frac{{v_0}^2 \\sin(2\\theta_0)}{g}$ and it shows that the maximum range depends on the square of initial velocity and the angle of launch. Let's assume that we don't know the phyiscs and we want to use regression to predict the maximum range. In our case, if the angle is between 50 and 60 degrees, we see maximum range and we get a maximum target 'y'. Similarly if the velocity is is between 90 and 100 arnitrary units, we observe the maximum range.\n",
    "\n",
    "The code cell right below creates the toy dataset you have to use to train and then test a regularized logistic regression model. Don't worry if you don't understand some parts of the code, it's using more advanced tools such as *pandas* and *scikit-learn*. Simply run the cell to obtain the \"x_train, x_test, y_train, y_test\" numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe86b474-3454-4fdb-ba83-417a99df6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of experiments\n",
    "n = 15000\n",
    "\n",
    "# Generate random angles between 10 and 90 degrees\n",
    "angles = np.random.uniform(low=10, high=90, size=n)\n",
    "\n",
    "# Generate random initial velocities\n",
    "velocities = np.random.uniform(low=10, high=100, size=n)\n",
    "\n",
    "# Combine angles and velocities into a single 2D array (this will be our input features)\n",
    "X = np.column_stack((angles, velocities))\n",
    "\n",
    "# Generate target variable\n",
    "# For simplicity, let's say a hit is when angle is between 50 and 60 degrees and velocity is between 90 and 100 units\n",
    "y = [1 if (50 <= angle <= 60) and (90 <= velocity <= 100) else 0 for angle, velocity in zip(angles, velocities)]\n",
    "\n",
    "# Convert y to a numpy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Balance the data sample: retain an equal number of hit and miss outcomes\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X, columns=['angle', 'velocity'])\n",
    "df['y'] = y\n",
    "df_sig = df[df['y']==1]\n",
    "df_back = df[df['y']==0].sample(n=df_sig.shape[0])\n",
    "df_balanced = pd.concat([df_sig, df_back])\n",
    "\n",
    "# Convert 'angle' and 'velocity' columns back to a 2D numpy array for X\n",
    "X = df_balanced[['angle', 'velocity']].values\n",
    "\n",
    "# Convert 'y' column back to a 1D numpy array for y\n",
    "y = df_balanced['y'].values\n",
    "\n",
    "# the code below prepares training and testing datasets for you (we will go over the sklearn package in the afternoon session)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=324, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44839042-f6fc-41b5-bd2b-888799b9ee49",
   "metadata": {},
   "source": [
    "In the cell below, define the cost function for a regularized logistic regression model that you will use to predict wether a given projectile hits the range or not. You should pass the function a regulaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eebf067-0139-4951-b671-a8bb903316fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Total: 40 points\n",
    "def sigmoid(z):\n",
    "    z = np.clip( z, -500, 500 )           \n",
    "    g = 1.0/(1.0+np.exp(-z))  ### correct sigmoid computation: 2/40\n",
    "\n",
    "    return g\n",
    "\n",
    "def compute_cost_logistic(X, y, w, b, lambda_ = 1): ### correct arguments: 5/40 (1 for each argument)\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape ### correct initialization: 4/40\n",
    "    cost = 0. ### correct initialization: 2/40\n",
    "    for i in range(m): ### loop for in proper range: 4/40\n",
    "        z_i = np.dot(X[i], w) + b                               \n",
    "        f_wb_i = sigmoid(z_i) ### correct z_i and sigmoid call: 4/40                                           \n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i) ### correct computation: 4/40     \n",
    "             \n",
    "    cost = cost/m ### correct computation: 2/40                                                     \n",
    "\n",
    "    reg_cost = 0 ### correct initialization: 2/40\n",
    "    for j in range(n): # ##loop for in proper range: 4/40      \n",
    "        reg_cost += (w[j]**2) ### correct sum: 2/40                                         \n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost ### correct computation: 3/40                             \n",
    "    \n",
    "    total_cost = cost + reg_cost                                  \n",
    "    return total_cost ### return of regularized cost: 2/40   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6a686-8036-4ae7-8d6f-20e8d4fee831",
   "metadata": {},
   "source": [
    "In the cell below, define a function that computes the gradient for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e667f41-1b8a-49af-b73a-ea4c557ca253",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Total: 40 points\n",
    "def compute_gradient_logistic(X, y, w, b): ### correct arguments: 4/40 (1 for each argument)\n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape ### correct initialization: 4/40\n",
    "    dj_dw = np.zeros((n,)) ### correct initialization: 4/40\n",
    "    dj_db = 0. ### correct initialization: 2/40\n",
    "\n",
    "    for i in range(m): ### loop for in proper range: 4/40   \n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b) ### correct computation: 4/40         \n",
    "        err_i  = f_wb_i  - y[i] ### correct computation: 2/40                       \n",
    "        for j in range(n): ### loop for in proper range: 4/40   \n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j] ### correct computation: 4/40     \n",
    "        dj_db = dj_db + err_i ### correct computation: 2/40 \n",
    "    dj_dw = dj_dw/m  ### correct computation: 2/40                                  \n",
    "    dj_db = dj_db/m  ### correct computation: 2/40                                  \n",
    "        \n",
    "    return dj_db, dj_dw ### correct return: 2/40 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7188f0-8bf1-4430-89fe-05b2f7d2bc04",
   "metadata": {},
   "source": [
    "In the cell below, define a function that implements the gradient descent for the logistic regression model above. **Hint**: to check wether the algorithm converges, you may want to print the cost funtion value every few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8c6752-e6b8-42c3-965d-fb63cb5c1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Total: 40 points\n",
    "import copy, math\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_): ### correct arguments: 7/40 (1 for each argument)  \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    " \n",
    "    J_history = [] ### correct initialization: 2/40\n",
    "    w = copy.deepcopy(w_in)  ### correct copy and/or initialization: 2/40\n",
    "    b = b_in ### correct initialization: 2/40\n",
    "    \n",
    "    for i in range(num_iters): ### loop for in proper range: 4/40   \n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b) ### correct function call: 6/40 (1 for each argument, 1 for each output)     \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw ### correct computation: 4/40                 \n",
    "        b = b - alpha * dj_db ### correct computation: 4/40                 \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b, lambda_) ) ### call of cost function: 5/40 (1 for each argument, appending to history is optional)   \n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")  ### printing cost function at intervals (nedded to check convergence) 2/40\n",
    "        \n",
    "    return w, b, J_history ### returning w and b (history is optional): 2/40 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ab634-5344-4346-bd7e-27d500a144d3",
   "metadata": {},
   "source": [
    "In the cell below, run the gradient descent for at least 10000 iterations to train the model. Use a regularization parameter lambda_ = 1.0, and a learning rate with a given value that gives you convergence. You may have to try a few times until you reach convergence. **Hint**: don't use a learning rate value smaller than $10^{-4}$, otherwise the convergence may become too slow with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6755cc-8173-48c8-beb7-c7c93da1d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.673978824643189   \n",
      "Iteration 1000: Cost 0.5283889039149194   \n",
      "Iteration 2000: Cost 0.5247808774233831   \n",
      "Iteration 3000: Cost 0.5212429145877682   \n",
      "Iteration 4000: Cost 0.5177738308900711   \n",
      "Iteration 5000: Cost 0.5143724324175792   \n",
      "Iteration 6000: Cost 0.5110375181872994   \n",
      "Iteration 7000: Cost 0.5077678823792483   \n",
      "Iteration 8000: Cost 0.5045623164728366   \n",
      "Iteration 9000: Cost 0.501419611281661   \n",
      "\n",
      "updated parameters: w:[-0.04573487  0.04296165], b:-0.5808417584099086\n"
     ]
    }
   ],
   "source": [
    "### Total: 15 points\n",
    "w_tmp  = np.zeros_like(x_train[0])\n",
    "b_tmp  = 0.\n",
    "alpha_ = 0.001 ### good value for convergence: 6/15\n",
    "lambda_ = 1.0 \n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, descent_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alpha_, iters, lambda_) ### correct call: 9/15 (1 for each argument, 1 for w_out, 1 for b_out)\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8fe2f-6ede-4148-8276-3c330a00d8b6",
   "metadata": {},
   "source": [
    "In the cell below, use your model to predict wether each of the entries in the 'x_test' array hits the target. You may then print the target values 'y_test' to see how well the model predicts the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de81d07a-f88e-4d70-8137-df7e27ec0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75463886 0.24935501 0.7450033  0.7484467  0.21515318]\n",
      "[1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "### Total: 15 points\n",
    "print(sigmoid(np.dot(x_test,w_out) + b_out)) ### correct: 13/15 points (dot product correct: 5/15, they add b_out 3/15, they actually call and print sigmoid 5/15)\n",
    "print(y_test) ### they print y_test as suggested: 2/12 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac32d8-7838-4d9f-a982-2ecbd6693984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
