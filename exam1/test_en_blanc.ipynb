{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dd4b91-7975-40b6-b5e3-d0f33522abf8",
   "metadata": {},
   "source": [
    "# Projectile Range\n",
    "Suppose we have a projectile with which we want to hit some range. The formula for the range of a projectile is $R = \\frac{{v_0}^2 \\sin(2\\theta_0)}{g}$ and it shows that the maximum range depends on the square of initial velocity and the angle of launch. Let's assume that we don't know the phyiscs and we want to use regression to predict the maximum range. In our case, if the angle is between 50 and 60 degrees, we see maximum range and we get a maximum target 'y'. Similarly if the velocity is is between 90 and 100 arnitrary units, we observe the maximum range.\n",
    "\n",
    "The code cell right below creates the toy dataset you have to use to train and then test a regularized logistic regression model. Don't worry if you don't understand some parts of the code, it's using more advanced tools such as *pandas* and *scikit-learn*. Simply run the cell to obtain the \"x_train, x_test, y_train, y_test\" numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe86b474-3454-4fdb-ba83-417a99df6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of experiments\n",
    "n = 15000\n",
    "\n",
    "# Generate random angles between 10 and 90 degrees\n",
    "angles = np.random.uniform(low=10, high=90, size=n)\n",
    "\n",
    "# Generate random initial velocities\n",
    "velocities = np.random.uniform(low=10, high=100, size=n)\n",
    "\n",
    "# Combine angles and velocities into a single 2D array (this will be our input features)\n",
    "X = np.column_stack((angles, velocities))\n",
    "\n",
    "# Generate target variable\n",
    "# For simplicity, let's say a hit is when angle is between 50 and 60 degrees and velocity is between 90 and 100 units\n",
    "y = [1 if (50 <= angle <= 60) and (90 <= velocity <= 100) else 0 for angle, velocity in zip(angles, velocities)]\n",
    "\n",
    "# Convert y to a numpy array\n",
    "y = np.array(y)\n",
    "\n",
    "# Balance the data sample: retain an equal number of hit and miss outcomes\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X, columns=['angle', 'velocity'])\n",
    "df['y'] = y\n",
    "df_sig = df[df['y']==1]\n",
    "df_back = df[df['y']==0].sample(n=df_sig.shape[0])\n",
    "df_balanced = pd.concat([df_sig, df_back])\n",
    "\n",
    "# Convert 'angle' and 'velocity' columns back to a 2D numpy array for X\n",
    "X = df_balanced[['angle', 'velocity']].values\n",
    "\n",
    "# Convert 'y' column back to a 1D numpy array for y\n",
    "y = df_balanced['y'].values\n",
    "\n",
    "# the code below prepares training and testing datasets for you (we will go over the sklearn package in the afternoon session)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=324, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44839042-f6fc-41b5-bd2b-888799b9ee49",
   "metadata": {},
   "source": [
    "In the cell below, define the cost function for a regularized logistic regression model that you will use to predict wether a given projectile hits the range or not. You should pass the function a regulaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eebf067-0139-4951-b671-a8bb903316fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "    g = 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    return g\n",
    "\n",
    "def compute_cost_logistic(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6a686-8036-4ae7-8d6f-20e8d4fee831",
   "metadata": {},
   "source": [
    "In the cell below, define a function that computes the gradient for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e667f41-1b8a-49af-b73a-ea4c557ca253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7188f0-8bf1-4430-89fe-05b2f7d2bc04",
   "metadata": {},
   "source": [
    "In the cell below, define a function that implements the gradient descent for the logistic regression model above. **Hint**: to check wether the algorithm converges, you may want to print the cost funtion value every few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8c6752-e6b8-42c3-965d-fb63cb5c1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b, lambda_) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ab634-5344-4346-bd7e-27d500a144d3",
   "metadata": {},
   "source": [
    "In the cell below, run the gradient descent for at least 10000 iterations to train the model. Use a regularization parameter lambda_ = 1.0, and a learning rate with a given value that gives you convergence. You may have to try a few times until you reach convergence. **Hint**: don't use a learning rate value smaller than $10^{-4}$, otherwise the convergence may become too slow with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6755cc-8173-48c8-beb7-c7c93da1d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.673978824643189   \n",
      "Iteration 1000: Cost 0.5283889039149194   \n",
      "Iteration 2000: Cost 0.5247808774233831   \n",
      "Iteration 3000: Cost 0.5212429145877682   \n",
      "Iteration 4000: Cost 0.5177738308900711   \n",
      "Iteration 5000: Cost 0.5143724324175792   \n",
      "Iteration 6000: Cost 0.5110375181872994   \n",
      "Iteration 7000: Cost 0.5077678823792483   \n",
      "Iteration 8000: Cost 0.5045623164728366   \n",
      "Iteration 9000: Cost 0.501419611281661   \n",
      "\n",
      "updated parameters: w:[-0.04573487  0.04296165], b:-0.5808417584099086\n"
     ]
    }
   ],
   "source": [
    "w_tmp  = np.zeros_like(x_train[0])\n",
    "b_tmp  = 0.\n",
    "alpha_ = 0.001\n",
    "lambda_ = 1.0\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, descent_history = gradient_descent(x_train, y_train, w_tmp, b_tmp, alpha_, iters, lambda_) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8fe2f-6ede-4148-8276-3c330a00d8b6",
   "metadata": {},
   "source": [
    "In the cell below, use your model to predict wether each of the entries in the 'x_test' array hits the target. You may then print the target values 'y_test' to see how well the model predicts the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de81d07a-f88e-4d70-8137-df7e27ec0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75463886 0.24935501 0.7450033  0.7484467  0.21515318]\n",
      "[1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(np.dot(x_test,w_out) + b_out))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac32d8-7838-4d9f-a982-2ecbd6693984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
